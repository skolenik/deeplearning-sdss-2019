---
title: "ARTIFICIAL INTELLIGENCE SECRET BOOTCAMP"
output: html_notebook
---

Load 'em packages!

```{r}
library(rpart)
# source of the kyphosis data set
library(rsample)
library(keras)
library(recipes)
library(tidyverse)
# source of the roc_auc()
library(yardstick)
options(yardstick.event_first = FALSE)
# the central piece
library(tensorflow)
```

Let's look at the data we're working with

```{r}
dplyr::glimpse(kyphosis)
```

We're going to predict whether kyphosis is present.

First, we'll perform an initial split into training/testing of the dataset.

```{r}
data_split <- initial_split(kyphosis, prop = 3/4, strata = 'Kyphosis')
training_data <- training(data_split)
testing_data <- testing(data_split)
```

Let's build our favorite classification model!

```{r}
logreg <- glm(Kyphosis ~ Age + Number + Start, data = training_data,
              family = binomial("logit"))
summary(logreg)
```

Create a data frame with the predictions.

```{r}
predicted <- testing_data %>% 
  mutate(present = predict(logreg, testing_data, type = "response"))
```

Calculate AUC

```{r}
roc_auc(predicted, Kyphosis, present)
roc_curve(predicted, Kyphosis, present) %>% autoplot()
```

## "Neural net"

```{r}
model1 <- keras_model_sequential() %>% 
  layer_dense(2,                         # dimensionality of the output, in this case these would be predicted probabilities
                 input_shape = 3,        # number of inputs; the bias term is added by default automatically
                 activation = "softmax") # shape of the unit function
model1

# model1 is modified by reference
model1 %>% 
  compile(loss = "binary_crossentropy",           # loss function to evaluate for the network; essentially log likelihood
          optimizer = optimizer_sgd (lr = 0.001), # stochastic gradient descent; other options pop up
          metrics = "accuracy")                   # 

```

Data prep

```{r}
x <- training_data %>% 
  select(Age, Number, Start) %>% 
  as.matrix()                     # the type has to be transformed to be fed to keras

# As categorical: Kyphosis - 1
y <- training_data$Kyphosis %>% 
  as.integer() %>% 
  `-`(1) %>% 
  to_categorical()                # keras function: creates dummy variables for both categories; Python convention to start indexing with 0
```

Fit the model

```{r}
model1 %>% fit(x = x, y = y, 
               batch_size = 32, # split the data set to compute the gradients on a batch per iteration rather the whole data set
               epochs = 100,    # number of training cycles
               verbose = 1)     # produces plots along the way
```

```{r}
predictions <- predict(model1, 
                       testing_data %>% 
                             select(Age, Number, Start) %>% 
                             as.matrix() # same transformation as for the training data
                       )
predicted <- testing_data %>% 
  mutate(present = predictions[,2])

roc_auc(predicted, Kyphosis, present)
```

Try adding more layers to make this perform better! (Yeah we're cheating by tuning on the testing set but that's OK no one has to know.)

## The data preprocessing that we should've done

Neural nets are easier to train when the predictors have similar magnitudes.

We'll transform our dataset using the `recipes` package. 
(This is useful so that both training and testing data have unified definitions.)

```{r}
rec <- recipe(Kyphosis ~ Age + Number + Start, data = training_data) %>% 
  step_dummy(Kyphosis) %>% 
  step_integer(Start) %>%
  step_center(Age, Number) %>% 
  step_scale(Age, Number) %>% 
  prep()
```

Note that we're going to treat `Start` as a categorical predictor here in order to demonstrate how to deal with them.

```{r}
start_num_levels <- training_data$Start %>%
  unique() %>% 
  length()

age <- layer_input(shape = 1, name = "age")         # this is a tensor of dimension 1, i.e., a vector
number <- layer_input(shape = 1, name = "number")
start <- layer_input(shape = 1, name = "start")

# `layer_embedding()` uses a lookup table to map each index
#   to a vector, and the vector values are learned during
#   training instead of specified beforehand. The reason for
#   `start_num_levels + 1` is so that we accommodate novel
#   levels in the factor at test time.
start_embedding <- layer_embedding(start, start_num_levels + 1, 4) %>%    # each level of the original predictor is converted to a dummy level vector
  layer_flatten()                                                         # make sure that these layers go together

kyphosis <- layer_concatenate(list(age, number, start_embedding)) %>%     # list of preceding layers
  layer_dense(8, activation = "relu") %>%                                 # go through layers
  layer_dense(2, activation = "softmax", name = "kyphosis")               # this will be the output (two probabilities)

model2 <- keras_model(
  inputs = list(age, number, start),
  outputs = kyphosis
)
```

Compile as before...

```{r}
model2 %>% compile(
  optimizer = optimizer_adam(lr = 1e-4),
  loss = "binary_crossentropy",
  metric = "accuracy"
)
```

Data prep is only slightly more involved

```{r}
prep_keras_data <- function(data) {
  list(
    age = data[, "Age", drop = FALSE],
    number = data[, "Number", drop = FALSE],
    start = data[, "Start", drop = FALSE]
  )
}
# need to have named lists with names matching the layer names

training_baked <- bake(rec, training_data, composition = "matrix")
x <- prep_keras_data(training_baked)
y <- training_baked[, 4] %>% to_categorical()
```

Training

```{r}
history <- model2 %>% 
  fit(x = x, y = y, batch_size = 32, epochs = 500, verbose = 1)
```

Evaluate

```{r}
predictions <- predict(
  model2,
  bake(rec, testing_data, composition = "matrix") %>%
    prep_keras_data()
)

predicted <- testing_data %>% 
  mutate(present = predictions[,2])

roc_auc(predicted, Kyphosis, present)
```
